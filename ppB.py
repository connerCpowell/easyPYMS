#import sys
from pyms.GCMS.IO.ANDI.Function import ANDI_reader
from pyms.GCMS.Function import build_intensity_matrix
from pyms.Noise.SavitzkyGolay import savitzky_golay
from pyms.Baseline.TopHat import tophat
from pyms.Deconvolution.BillerBiemann.Function import BillerBiemann, rel_threshold, num_ions_threshold
from pyms.Peak.Class import Peak
from pyms.Peak.Function import peak_sum_area
from pyms.Experiment.Class import Experiment
from pyms.Experiment.IO import store_expr
import itertools
import fnmatch
import os
from pyms.Noise.Analysis import window_analyzer
import csv
import argparse

#import re

from datetime import datetime



def glob(glob_pattern, directoryname, splitPattern):
    '''
    Walks through a directory and its subdirectories looking for files matching
    the glob_pattern and returns a list.

    :param directoryname: Any accessible folder name on the filesystem.
    :param glob_pattern: A string like "*.txt", which would find all text files.
    :return: A list of absolute filepaths matching the glob pattern.
    '''
    matches = []
    names = []
    for root, dirnames, filenames in os.walk(directoryname):
        for filename in fnmatch.filter(filenames, glob_pattern):
            absolute_filepath = os.path.join(root, filename)
            matches.append(absolute_filepath)

            name = filename.rsplit(splitPattern)[-1]
            names.append(name)

    print('n1', names)
    return matches, names


def matrix_from_cdf(cdffile, name):
    '''
    Intakes a .cdf file and produces an intensity matrix and a noise level .
    The noise level info is obtained by producing a tic and using the window_analyzer
    method to extract a noise approximation.

    @param cdffile: Absolutepath to a .cdf file to be processed
    @param name: file name associated with .cdf file
    @return: An intensity matrix and a corresponding noise level value
    '''
    data = ANDI_reader(cdffile)
    print(name)
    data.info()
    tic = data.get_tic()
    noise_lvl = window_analyzer(tic)
    print('nz=', noise_lvl)

    return build_intensity_matrix(data), noise_lvl


def Preprocess_IntensityMatrixes(matrixes):
    '''
    noise removal and baseline correction of Intensity Matricies
    input matrix list, outputs corrected/"cleansed" matrix list

    @param matrixes: List of matrixes generated by the matrix_from_cdf method
    @return: List of matrixes that have been 'cleansed'
    '''

    count = 1
    for im in matrixes:

        n_s, n_mz = im.get_size()
        count += 1

        for ii in range(n_mz):
            # print("Working on IC#", ii+1, " Unit", count)
            ic = im.get_ic_at_index(ii)
            ic_smoof = savitzky_golay(ic)
            ic_bc = tophat(ic_smoof, struct='1.5m')
            im.set_ic_at_index(ii, ic_bc)

    # print(matrixes)
    return (matrixes)  # save to file


def Peak_detector(pp_im, noise, name, points, scans, percent, ni, name_tag, sdir):
    # Peak detection and filtering and selection
    peakz = []
    # counter = 1
    savePath = sdir
    ms_data_files = []

    print("len pp_im", len(list(pp_im)))
    print("len noise", len(noise))
    print("len name", len(name), name)

    for im, n, na in itertools.izip(list(pp_im), noise, name):

        #ms_data = []

        # print(na)
        poss_peaks = BillerBiemann(im, points=points, scans=scans)  # increase scan #
        pi = rel_threshold(poss_peaks, percent=percent)
        nin = num_ions_threshold(pi, n=ni, cutoff=n)

        completeName = os.path.join(savePath, na + name_tag + "ms_data.csv")
        with open(completeName, 'w') as f:
            w = csv.writer(f)
            head = ['Area', 'RTs'] + [float(i) for i in range(35,221)]


            w.writerow(head)
            for peak in nin:

                area = peak_sum_area(im, peak)
                # print('area:', area)
                peak.set_area(area)
                ms = peak.get_mass_spectrum()
                # print("Peaks rt: ", peak.get_rt())
                # print("Peaks ms_list: ", ms.mass_list)
                # print("Peaks ms_spec: ", list(ms.mass_spec))
                p_rt = peak.get_rt()
                its = []
                ms_items = list(ms.mass_spec)
                for spec in ms_items:
                    f_spec = float(spec)
                    its.append(f_spec)

                ms_d = ([area] + [p_rt] + its)
                # ms_d = its
                # print('ms_d', ms_d)
                w.writerow(ms_d)

            f.close()

        peakz.append(nin)
        # #print("...", counter)
        # counter += 1
        ms_data_files.append(completeName)
    print('ms_data_files:', ms_data_files)

    return [peakz, ms_data_files]


def replacementWriter(ll, pp, chars):
    ll = str(ll)
    for c in chars:
        ll = ll.replace(c, '')
    pp.write(ll + "\n")


def MS_process(file_list):
    '''
    @param file_list:
    @return:
    '''

    ratio_set = []
    print(file_list)

    for n in file_list:
        peaks = []
        areas = []
        print('n=', n)
        #print("-------------11------------------------------------")
        with open(n, 'r') as f:
            next(f)
            for line in f:
                sline = line.split(',')
                a = sline[0]
                p = sline[1]
                # p2 = sline[2]
                # p3 = sline[3]
                # print('sline=', sline)
                # print('are2a=', a)
                # print('peak rt2=', p)

                sline.pop(0)
                sline.pop(0)
                peaks.append(p)
                areas.append(a)

                ratios = []
                maxi = max(map(float, sline))
                # print('maxi=', maxi)
                # loc = sline.index(str(maxi))

                c = 34.0
                for i in sline:
                    r = float(i) / float(maxi)
                    rx = int(r * 999)
                    ratios.append([c, rx])
                    c += 1

                ratio_set.append(ratios)
            # print('peaks=', peaks)
            # print('areas=', areas)

            #print("----------22--------------------------------------")
            name1 = n.rsplit('ms_data.csv')
            name2 = name1[0] + '.txt'
            print('n2=', name2)

            with open(name2, "w+") as pp:
                for ratios, p, a, in zip(ratio_set, peaks, areas):
                    replacementWriter(ll=('Name:', p, 'Area-', a), pp=pp, chars="',()")
                    replacementWriter(ll=('Num Peaks:', 10), pp=pp, chars="',()")

                    for i in sorted(ratios, key=lambda t: t[1], reverse=True)[:10]:
                        replacementWriter(ll=i, pp=pp, chars="[],")
                    pp.write("\n")




            # ms_name = os.path.join()
            # print('ms', name1)

            # pp = open(name2, "w+")
            # for ratios, p, a in zip(ratio_set, peaks, areas):
            #     # print('ratios=', ratios)
            #     rs = sorted(ratios, key=lambda t: t[1], reverse=True)[:10]
            #
            #     # print('rs=', rs)
            #     ll = 'Name:', p, 'Area-', a
            #     ll = str(ll).replace("'", "").replace(",", "").replace("(", "").replace(")", "")
            #     # print('ll=', ll)
            #     pp.write(ll + "\n")
            #
            #     nn = 'Num Peaks:', 10
            #     nn = str(nn).replace("'", "").replace(",", "").replace("(", "").replace(")", "")
            #     # print('nn=', nn)
            #     pp.write(nn + "\n")
            #     for i in rs:
            #         ss = str(i).replace('[', '').replace(']', '').replace(',', '')
            #         # print('ss=', ss)
            #         pp.write(ss + "\n")
            #     # print('\n')
            #     pp.write("\n")
            # pp.close()



            # nameFil = name1[0] + '.FIL'
            # ff = open(nameFil, "w+")
            # nameMSD = "C:\mymsds\data\be-31a1.MSD"
            # # ff.write(name2 + " OVERWRITE")
            # ff.write(name2 + " APPEND")
            #
            # ff.close()


def Experiment_store(names, peakz, name_tag, sdir2):
    for n, p in itertools.izip(names, peakz):
        expr = Experiment(n, p)
        expr.sele_rt_range(["1m", "50m"])
        store_expr(sdir2 + n + name_tag + ".expr", expr)
        print(n, "checked")


def main():
    parser = argparse.ArgumentParser(description="Preprocessing & Peak detection tool for GC-MS data")

    parser.add_argument("-f",
                        "--CDFs",
                        action="store",
                        dest="dirc",
                        nargs="?",
                        type=str,
                        default="tmp/",
                        help="CDF Directory: Location of .cdf files to be processed \n")

    parser.add_argument("-n",
                        "--name",
                        action="store",
                        nargs="?",
                        type=str,
                        #default="",
                        help="Name Split: Where to split .cdf file name \n",
                        dest="name"
                        )

    parser.add_argument("-p",
                        "--points",
                        action="store",
                        nargs="?",
                        # action='store_const',
                        const=1,
                        type=int,
                        default=140,
                        help="Points: Number of points used to determine window size \n",
                        dest="points",
                        )

    parser.add_argument("-s",
                        "--scans",
                        action="store",
                        dest="scans",
                        nargs="?",
                        const=1,
                        type=int,
                        default=25,
                        help="Scans: Number of scans to average for \n")

    parser.add_argument("-t",
                        "--threshold",
                        action="store",
                        dest="threshold",
                        nargs="?",
                        const=1,
                        type=int,
                        default=3,
                        help="Threshold percent: Minimum threshold percentage to be considered a peak \n")

    parser.add_argument("-i",
                        "--ion",
                        action="store",
                        dest="ion",
                        nargs="?",
                        const=1,
                        type=int,
                        default=3,
                        help="Number of Ions: Minimum number of Ions required for peak consideration \n")

    parser.add_argument("-c",
                        "--CSVdir",
                        action="store",
                        dest="sdir",
                        nargs="?",
                        type=str,
                        help="CSV Directory: Location to save MS extraction .csv files \n")

    parser.add_argument("-e",
                        "--EXPRdir",
                        action="store",
                        dest="sdir2",
                        help="EXPR Directory: Location to save the .expr files for alignment scripts \n")
    args = parser.parse_args()

    print(args)

    name_tag = str('p' + str(args.points) + 's' + str(args.scans) + '%' + str(args.threshold) + 'n' + str(args.ion))

    print("CDF file directory:", args.dirc)
    print("split:", args.name)
    print("Points:", args.points)
    print("Scans:", args.scans)
    print("Percent:", args.threshold)
    print("num. of ions:", args.ion)
    print("Name_tag:", name_tag)
    print("Storage directory (csv):", args.sdir)
    print("Storage dir (expr):", args.sdir2)

    matrixes = []
    noise = []

    startTime = datetime.now()

    # Glob command used to locate .cdf files and create a list of the files
    list_of_cdffiles, names = glob(glob_pattern='*.cdf', directoryname=args.dirc, splitPattern=args.name)

    for cdffile, name in itertools.izip(list_of_cdffiles, names):
        print('name=', name)
        # names.append(name)
        m_c = matrix_from_cdf(cdffile, name)
        matrixes.append(m_c[0])
        noise.append(m_c[1])

    print('names=', names)

    pp_im = Preprocess_IntensityMatrixes(matrixes)
    # for i, n in itertools.izip(pp_im, noise):
    #     print(i, n)
    peak_m = Peak_detector(pp_im, noise, names, args.points, args.scans, args.threshold, args.ion, name_tag, args.sdir)
    Experiment_store(names, peak_m[0], name_tag, args.sdir2)

    print('p1=', peak_m[1])
    MS_process(peak_m[1])

    timeR = datetime.now() - startTime
    #tiM = timeR/60
    print('Seconds:', str(timeR))
    #print('Minutes', str(tiM))

    # print(args.dirc, points)


if __name__ == "__main__":
    main()
